"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9577],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=c(n),d=s,f=p["".concat(l,".").concat(d)]||p[d]||m[d]||r;return n?a.createElement(f,i(i({ref:t},u),{},{components:n})):a.createElement(f,i({ref:t},u))}));function d(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=n.length,i=new Array(r);i[0]=p;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:s,i[1]=o;for(var c=2;c<r;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},1917:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return l},default:function(){return d},frontMatter:function(){return o},metadata:function(){return c},toc:function(){return m}});var a=n(7462),s=n(3366),r=(n(7294),n(3905)),i=["components"],o={sidebar_position:5,title:"Custom Dataset"},l=void 0,c={unversionedId:"getting-started/customizing-things/custom-dataset",id:"getting-started/customizing-things/custom-dataset",title:"Custom Dataset",description:"Implementing your custom dataset with classy is easy. You just need to subclass BaseDataset:",source:"@site/docs/getting-started/customizing-things/custom-dataset.md",sourceDirName:"getting-started/customizing-things",slug:"/getting-started/customizing-things/custom-dataset",permalink:"/classy/docs/getting-started/customizing-things/custom-dataset",editUrl:"https://github.com/sunglasses-ai/classy/edit/main/docs/docs/getting-started/customizing-things/custom-dataset.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"Custom Dataset"},sidebar:"tutorialSidebar",previous:{title:"Custom Model",permalink:"/classy/docs/getting-started/customizing-things/custom-model"},next:{title:"Custom Optimizer",permalink:"/classy/docs/getting-started/customizing-things/custom-optimizer"}},u={},m=[{value:"A Minimal Example",id:"a-minimal-example",level:2}],p={toc:m};function d(e){var t=e.components,n=(0,s.Z)(e,i);return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Implementing your custom dataset with classy is easy. You just need to subclass BaseDataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"class MyCustomDataset(BaseDataset):\n    @staticmethod\n    def requires_vocab() -> bool:\n        # returns true if the dataset requires fitting a vocabulary, false otherwise\n        pass\n\n    @staticmethod\n    def fit_vocabulary(samples: Iterator[ClassySample]) -> Vocabulary:\n        # fits the vocabulary\n        pass\n\n    def __init__(self, *args, **kwargs):\n        # construct fields batchers\n        fields_batchers = {...}\n        super().__init__(*args, fields_batchers=fields_batchers, **kwargs)\n\n    def dataset_iterator_func(self) -> Iterable[Dict[str, Any]]:\n        # yields a sequence of dictionaries, each representing a sample\n        pass\n")),(0,r.kt)("p",null,"The underlying flow is as follows:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dataset instantiation is transparent to you, and takes place via 1 of 3 class methods:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"BaseDataset.from_file"),(0,r.kt)("li",{parentName:"ul"},"BaseDataset.from_lines"),(0,r.kt)("li",{parentName:"ul"},"BaseDataset.from_samples"))),(0,r.kt)("li",{parentName:"ul"},"Regardless on which one is invoked, BaseDataset exposes to you a ",(0,r.kt)("em",{parentName:"li"},"samples_iterator")," function that, once invoked, returns a sequence of classy samples"),(0,r.kt)("li",{parentName:"ul"},"In your ",(0,r.kt)("em",{parentName:"li"},"dataset_iterator_func"),", you iterate on these samples and convert them to dictionary objects"),(0,r.kt)("li",{parentName:"ul"},"These dictionary-like samples are then batched using the ",(0,r.kt)("em",{parentName:"li"},"fields_batchers")," variable you pass to BaseDataset in your ",(0,r.kt)("em",{parentName:"li"},"_","_","init","_","_"),"; it is essentially a dictionary mapping\nkeys in your dictionary-like samples to collating functions")),(0,r.kt)("h2",{id:"a-minimal-example"},"A Minimal Example"),(0,r.kt)("p",null,"Practically, imagine you want to build your own SequenceDataset for BERT."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="classy.data.dataset.my_bert_sequence_dataset.py"',title:'"classy.data.dataset.my_bert_sequence_dataset.py"'},"from transformers import AutoTokenizer\nfrom classy.data.data_drivers import SequenceSample\nfrom classy.data.dataset.base import batchify, BaseDataset\n\n\nclass MyBertSequenceDataset(BaseDataset):\n    pass\n")),(0,r.kt)("p",null,"You first deal with the vocabulary methods. As you are doing sequence classification, you need to fit the label vocabulary:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'@staticmethod\ndef requires_vocab() -> bool:\n    return True\n\n\n@staticmethod\ndef fit_vocabulary(samples: Iterator[SequenceSample]) -> Vocabulary:\n    return Vocabulary.from_samples(\n        [{"labels": sample.reference_annotation} for sample in samples]\n    )\n')),(0,r.kt)("p",null,"Then, define your constructor and, in particular, your ",(0,r.kt)("em",{parentName:"p"},"fields_batchers"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def __init__(\n    self,\n    samples_iterator: Callable[[], Iterator[SequenceSample]],\n    vocabulary: Vocabulary,\n    transformer_model: str,\n    tokens_per_batch: int,\n    max_batch_size: Optional[int],\n    section_size: int,\n    prebatch: bool,\n    materialize: bool,\n    min_length: int,\n    max_length: int,\n    for_inference: bool,\n):\n\n    # load bert tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(\n        transformer_model, use_fast=True, add_prefix_space=True\n    )\n\n    # define fields_batchers\n    fields_batcher = {\n        "input_ids": lambda lst: batchify(\n            lst, padding_value=self.tokenizer.pad_token_id\n        ),\n        "attention_mask": lambda lst: batchify(lst, padding_value=0),\n        "labels": lambda lst: torch.tensor(lst, dtype=torch.long),\n        "samples": None,\n    }\n\n    super().__init__(\n        samples_iterator=samples_iterator,\n        vocabulary=vocabulary,\n        batching_fields=["input_ids"],\n        tokens_per_batch=tokens_per_batch,\n        max_batch_size=max_batch_size,\n        fields_batchers=fields_batcher,\n        section_size=section_size,\n        prebatch=prebatch,\n        materialize=materialize,\n        min_length=min_length,\n        max_length=max_length if max_length != -1 else self.tokenizer.model_max_length,\n        for_inference=for_inference,\n    )\n')),(0,r.kt)("p",null,"Finally, you need to implement the ",(0,r.kt)("em",{parentName:"p"},"dataset_iterator_func"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def dataset_iterator_func(self) -> Iterable[Dict[str, Any]]:\n    # iterate on samples\n    for sequence_sample in self.samples_iterator():\n        # invoke tokenizer\n        input_ids = self.tokenizer(sequence_sample.sequence, return_tensors="pt")[\n            "input_ids"\n        ][0]\n        # build dict\n        elem_dict = {\n            "input_ids": input_ids,\n            "attention_mask": torch.ones_like(input_ids),\n        }\n        if sequence_sample.reference_annotation is not None:\n            # use vocabulary to convert string labels to int labels\n            elem_dict["labels"] = [\n                self.vocabulary.get_idx(\n                    k="labels", elem=sequence_sample.reference_annotation\n                )\n            ]\n        elem_dict["samples"] = sequence_sample\n        yield elem_dict\n')))}d.isMDXComponent=!0}}]);